{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_mnist_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babai95/Fashion-Mnist-Classification-using-Tensorflow/blob/master/Fashion_mnist_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-xN2vGpsf-",
        "colab_type": "text"
      },
      "source": [
        "Fashion MNIST classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AADjqbqlppkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH9BxfZtqiPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88lPipeYs-i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-Shirt/Top','Trouser','PullOver','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7aet9QqvE4o",
        "colab_type": "text"
      },
      "source": [
        "Class_names is a list .these are the classes and train_labels are the labels.\n",
        "label 0 : T-shirt/Top \n",
        "1: Trouser\n",
        "2: Pullovers\n",
        "3: Dress\n",
        "4: Coat\n",
        "5: Sandal 6. Shirt 7. Sneaker 8. Bag 9. Ankle Boot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNZKxM7kuAVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w23hpW8luhuD",
        "colab_type": "text"
      },
      "source": [
        "shape is (60000, 28,28). This means there are 60,000 images and each image is of size 28*28 pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbVX67mGuIg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QVlCVXFuz38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOL593aIu3X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C__WkYlkrr6",
        "colab_type": "text"
      },
      "source": [
        "train_labels is an array of size 60000. the 1st value is 9 , which means that the 1st image of train_images is an ankle boot, whose label is 9\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW89Ihd_kpoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print the 1st image\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.gca().grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y62u-cMCFv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Image is: \")\n",
        "print(class_names[train_labels[0]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zka2kA-tEmrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing the data\n",
        "# 1st step is normalizing the data in the range 0 to 1 before feeding the data to the neural network model, as neural network prefers data to be in the range 0 to 1\n",
        "\n",
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0\n",
        "#We are normalizing the pixel values which are in the range 0 to 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeVGuaa5TLGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display the 1st 25 images from the training set and the class_names of each image\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qz0HNjM8BJw",
        "colab_type": "text"
      },
      "source": [
        "Now we are building the neural network model. We use sequential api of keras, which uses a stack of layers and it is used in those cases where there is single i/p and single o/p, like here we will give one image as i/p and get its corresponding label as o/p\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7yVxWxyj5lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSobjzas46Z",
        "colab_type": "text"
      },
      "source": [
        "Here in Sequential api, we are using a stack of layers. \n",
        "\n",
        "1st Layer: Flatten Layer: To make it simple, we flatten the 2d image into 1d vector of size 28*28. This is just preprocessing and no ML\n",
        "\n",
        "2nd Layer: Dense Layer: It has 128 nodes or neurons and for such simple classification problems, we use relu as activation functions for all the middle layers(here only 1 middle layer)\n",
        "\n",
        "3rd Layer : Dense Layer. This is the last layer, which is the o/p layer and it has 10 nodes(from 0 to 9, it will output, so 10 nodes). We want to give o/p as probabilities and the node with the highest probability will be the final predicted label. As we are using probabilities, so we will use softmax as activation func.\n",
        "\n",
        "In neural networks, if we increase the no. of layers, then network will learn more patterns and if no. of units or nodes in each layer is increased, it will learn more no. of types in each pattern, eg, in convolutional neural network, one layer learns faces, the 2nd layer learns edges and in 2nd layer if more no. of nodes are there, then it will learn more different types of edges. So, overall learning improves with large no. of layers and large no. of units in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOlUEoTTAu-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwoVi8rhzyzT",
        "colab_type": "text"
      },
      "source": [
        "While compiling, we use 2 things, optimizer and loss function. We use AdamOptimizer() as optimizer and sparse_categorical_crossentropy as loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPXfnzN3zxGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fit or train the model for training set. Fit is another word for train\n",
        "model.fit(train_images,train_labels,epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mz4Z6IIIoBy",
        "colab_type": "text"
      },
      "source": [
        "Epoch means one pass over the entire training set to learn from the training set. If no. of epochs are very less, the machine wont learn the patterns properly. If the no. of epochs are very high, it would cause overfitting, i.e it will learn the patterns to such a extent that it will simply memorize the patterns in training set, and in test set, since that exact pattern will not be there, so it will not predict correctly and give bad results for test set. So, the no. of epochs needs to be optimal. \n",
        "\n",
        "So, after training the model, we got 2 things: loss for training set and accuracy for training set.\n",
        "Lower loss and higher accuracy indicates training was good. The overall loss and accuracy are the ones on the last epoch.Training set accuracy is 92.03%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnzMu7CNIUnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate the model on test set to know how the model performs\n",
        "test_loss,test_acc = model.evaluate(test_images,test_labels)\n",
        "print(\"Test accuracy: \", test_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7DEwAvxPhcG",
        "colab_type": "text"
      },
      "source": [
        "If the test set accuracy is almost similar to training set accuracy, then there is a balance in the no. of epochs selected i.e right epoch selected.  Here Test set accuracy is 87.48%. So, test set accuracy almost similar to training set accuracy. if test set accuracy << training set accuracy, then that is the case of overfitting, where model  performs worse in test set that in training set.  And if both test set accuracy and training set accuracy are very less, then that's the case of underfitting and learning needs to be done more properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq2XxpwZPWM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make predictions\n",
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kqHL9i-UYt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkmG97-hW-6b",
        "colab_type": "text"
      },
      "source": [
        "predictions is an array . prediction[0] is for 1st test set image and it contains probability distributions of 10 labels from 0 to 9 and we can see that probability distribution is maximum for label 9, so its final predicted value is 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxrWtM4FUhd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(25):\n",
        "  print(\"prediction : \", np.argmax(predictions[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHU11gD1zBNZ",
        "colab_type": "text"
      },
      "source": [
        "So, here np.argmax(predictions[0]) gives the max value out of all the values in predictions[0] and that is the overall prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uF5cLSQY6v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}